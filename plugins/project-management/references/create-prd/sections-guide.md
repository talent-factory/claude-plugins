# PRD Abschnitte: Detaillierter Guide

Detaillierte Anleitung f√ºr jeden Abschnitt eines PRD mit Best Practices, Beispielen und h√§ufigen Fehlern.

## Executive Summary

### Zweck

Busy Stakeholder in 30 Sekunden den Kern verstehen lassen.

### Struktur (3-5 S√§tze)

1. **Was** wird gebaut?
2. **F√ºr wen** ist es?
3. **Warum** ist es wichtig?
4. **Erwarteter Impact** (Business/User)
5. **Wann** (Timeline)

### Beispiel: Gut ‚úÖ

```markdown
## Executive Summary

Diese PRD beschreibt ein KI-gest√ºtztes Budgetierungs-Feature f√ºr
FinanceApp, das Nutzern automatische Ausgaben-Kategorisierung und
Budget-Vorschl√§ge bietet. Prim√§re Zielgruppe sind die 2.5M aktiven
Nutzer, die manuell kategorisieren (70% unserer Nutzerbasis).
Feature adressiert #1 User-Pain-Point aus letztem Research und soll
User-Engagement um 25% steigern und Onboarding-Drop-off um 15%
reduzieren. Launch geplant f√ºr Q2 2024 nach 8-w√∂chiger Entwicklung.
```

### Beispiel: Schlecht ‚ùå

```markdown
## Executive Summary

Wir bauen ein neues Feature. Es wird cool sein und Nutzer werden
es m√∂gen. Wir starten bald.
```

### H√§ufige Fehler

- ‚ùå Zu lang (> 1 Absatz)
- ‚ùå Zu vage (keine konkreten Zahlen)
- ‚ùå Zu technisch (Implementation-Details)
- ‚ùå Fehlende Business-Metrics

---

## Problemstellung

### Zweck

Klarheit schaffen WARUM wir bauen. Alle Stakeholder m√ºssen Problem verstehen.

### Struktur

1. **Aktueller Zustand**: Wie ist es jetzt?
2. **Problembeschreibung**: Was ist das Problem?
3. **Auswirkungen**: Wer ist betroffen? Wie stark?
4. **Evidenz**: Daten, die Problem belegen
5. **Opportunit√§t**: Warum jetzt l√∂sen?

### Schreibtipps

**Konkret, nicht abstrakt**:
- ‚úÖ "Nutzer verbringen durchschnittlich 12 Minuten mit manueller Kategorisierung"
- ‚ùå "Kategorisierung ist zeitaufw√§ndig"

**Quantifiziert**:

- ‚úÖ "45% der Nutzer brechen Onboarding bei Kategorisierung ab"
- ‚ùå "Viele Nutzer brechen Onboarding ab"

**Nutzer-Zitate einbinden**:

```markdown
"Ich hasse es, jede Transaktion manuell zu kategorisieren.
Das dauert ewig!" - Sarah, 32, Freelancerin
```

### Template

```markdown
## Problemstellung

### Aktueller Zustand

[Beschreibe Status Quo neutral]

Aktuell m√ºssen Nutzer alle Transaktionen manuell kategorisieren.
Die App bietet keine automatischen Vorschl√§ge. Neue Nutzer sehen
eine leere Liste und m√ºssen Kategorien von Grund auf aufbauen.

### Das Problem

[Spezifisches Problem identifizieren]

**F√ºr Nutzer**:
- Zeitintensiv: √ò 12 Minuten/Tag f√ºr Kategorisierung
- Frustrierend: Repetitive, manuelle Arbeit
- Error-prone: 23% falsche Kategorisierungen

**F√ºr Business**:
- Onboarding-Abbruch: 45% brechen bei Kategorisierung ab
- Support-Last: 300 Tickets/Monat zu "Kategorisierung"
- Churn-Risiko: 18% der Churned nennen "zu kompliziert"

### Evidenz

**Quantitative Daten**:
- Analytics: Nur 30% nutzen Kategorisierung regelm√§√üig
- Time-on-Task: √ò 12 Min. f√ºr 20 Transaktionen
- Error Rate: 23% m√ºssen re-kategorisiert werden

**Qualitative Forschung**:
- User Interviews (n=50): 94% w√ºnschen Auto-Kategorisierung
- Support-Feedback: "Kategorisierung" ist #1 Beschwerde
- NPS-Comments: 67 negative Mentions (letzte 3 Monate)

**Competitive Analysis**:
- 5/5 Top-Competitor haben Auto-Kategorisierung
- Market Standard seit 2022
- User erwarten dieses Feature

### Warum jetzt?

**Strategic Timing**:
- Q2 Marketing-Kampagne: 500k neue Nutzer erwartet
- Onboarding-Optimierung ist Q1 OKR
- Competitor X launcht √§hnliches Feature Q3

**Technical Readiness**:
- ML-Modell bereits trainiert (Accuracy: 89%)
- Infrastructure vorhanden
- Design-System hat Components

**Business Impact**:
- Projected Revenue Impact: +‚Ç¨250k/Jahr (15% Churn-Reduktion)
- ROI: Break-even nach 4 Monaten
```

### H√§ufige Fehler

- ‚ùå L√∂sung statt Problem beschreiben
- ‚ùå Keine Daten zur Untermauerung
- ‚ùå Problem zu klein (nicht worth building)
- ‚ùå Problem zu gro√ü (nicht solvable)

---

## Ziele & Erfolgsmetriken

### Zweck

Definieren wie wir Erfolg messen. Basis f√ºr Post-Launch-Review.

### SMART Ziele

- **S**pezifisch: Klar was gemessen wird
- **M**essbar: Quantifizierbar
- **A**rreichbar: Realistisch
- **R**elevant: Wichtig f√ºr Business/User
- **T**erminiert: Klarer Zeitrahmen

### Struktur

```markdown
## Ziele & Erfolgsmetriken

### Produkt-Ziele

1. **Benutzer-Engagement verbessern**
   - Mehr Nutzer verwenden Kategorisierung regelm√§√üig
   - L√§ngere Session-Dauer auf App
   - H√∂here Feature-Discovery

2. **Onboarding-Erlebnis optimieren**
   - Schnelleres Onboarding
   - Weniger Abbr√ºche
   - Bessere First-Time-UX

3. **Support-Last reduzieren**
   - Weniger Support-Tickets
   - Self-Service-Rate erh√∂hen

### Business-Ziele

- **Revenue**: Churn-Reduktion ‚Üí +‚Ç¨250k ARR
- **Kosten**: Support-Kosten um ‚Ç¨50k/Jahr reduzieren
- **Wachstum**: Onboarding-Completion-Rate steigern

### Erfolgsmetriken

#### Prim√§re Metriken (Launch + 4 Wochen)

| Metrik | Beschreibung | Baseline | Target | Messung |
|--------|--------------|----------|--------|---------|
| **Feature Adoption** | % Nutzer die Auto-Kategorisierung aktivieren | 0% | 60% | Analytics Event |
| **Categorization Accuracy** | % korrekt kategorisierte Transaktionen | 77% (manual) | 85% | User Confirmation Rate |
| **Time-to-Categorize** | √ò Zeit f√ºr 20 Transaktionen | 12 Min. | < 2 Min. | Event Timing |

#### Sekund√§re Metriken (Launch + 8 Wochen)

| Metrik | Beschreibung | Baseline | Target | Messung |
|--------|--------------|----------|--------|---------|
| **Onboarding Completion** | % die Onboarding abschlie√üen | 55% | 70% | Funnel Analysis |
| **Support Tickets** | Tickets zu "Kategorisierung" | 300/Monat | < 150/Monat | Zendesk Tags |
| **NPS Impact** | NPS-Score Verbesserung | 35 | 40+ | In-App Survey |
| **Daily Active Users** | % DAU die Feature nutzen | N/A | 40% | Analytics |

#### Guardrail Metriken

Metrics die NICHT negativ beeinflusst werden d√ºrfen:

- Performance: App-Ladezeit bleibt < 3s
- Accuracy: Kategorisierung-Fehler nicht > 20%
- Privacy: Keine User-Complaints zu Datennutzung

### Tracking-Plan

**Analytics Events**:
```javascript
// Feature Activation
track('auto_categorization_enabled', {
  user_id: string,
  timestamp: datetime
})

// Kategorisierung
track('transaction_auto_categorized', {
  transaction_id: string,
  category: string,
  confidence: float,
  user_confirmed: boolean
})

// Korrekturen
track('category_corrected', {
  transaction_id: string,
  old_category: string,
  new_category: string
})
```

**Dashboard**: Link to Mixpanel Dashboard

**Review Schedule**:
- Week 1: Daily Review (PM)
- Week 2-4: Weekly Review (PM + Eng Lead)
- Week 8: Comprehensive Post-Mortem (All Stakeholders)
```

### H√§ufige Fehler

- ‚ùå Vage Ziele ("mehr Nutzer")
- ‚ùå Zu viele Metriken (Focus verlieren)
- ‚ùå Nicht messbare Ziele ("Nutzer gl√ºcklicher")
- ‚ùå Keine Baseline f√ºr Vergleich
- ‚ùå Unrealistische Targets

---

## User Stories & Personas

### Zweck

Empathie f√ºr Nutzer schaffen. Team versteht WER nutzt WAS WARUM.

### Personas erstellen

**Basiert auf echten Daten**:
- User Research
- Analytics Segmentation
- Support-Feedback
- Sales/CS Input

### Persona Template

```markdown
## Prim√§re Personas

### Persona 1: "Budget-Bewusste Sarah"

![Persona Image or Icon]

**Demographie**:
- Alter: 28-35
- Beruf: Freelancerin, Wissensarbeit
- Einkommen: ‚Ç¨3.000-4.500/Monat (variabel)
- Tech-Affinit√§t: Hoch
- Standort: Urban, Deutschland

**Kontext & Verhalten**:
- Verwendet 5+ Finance-Apps
- Checkt Finanzen t√§glich (Routine)
- Priorisiert Automatisierung
- Mobile-First User (80% Mobile)

**Pain Points**:
1. "Ich habe keine Zeit f√ºr manuelle Arbeit"
2. "Ich brauche schnellen √úberblick"
3. "Kategorisierung ist repetitiv und nervig"

**Ziele mit App**:
- Automatischer Finanz-√úberblick
- Wenig Maintenance
- Insights ohne Arbeit

**Quote**:
> "Ich will morgens beim Kaffee schnell checken k√∂nnen, ob ich
> on-track bin. Nicht 10 Minuten Transaktionen kategorisieren."

**Wie misst man Erfolg f√ºr Sarah?**:
- Time-to-Insight < 30 Sekunden
- T√§glich App-Nutzung
- Hohe NPS-Score

**Segment Size**: 40% unserer User-Base (~1M Nutzer)
```

### User Story Format

```markdown
## User Stories

### Epic 1: Automatische Kategorisierung

#### US-1.1: Erste automatische Kategorisierung

**Priorit√§t**: Must-Have
**Story Points**: 5
**Sprint**: 1

**Als** Budget-bewusste Sarah
**M√∂chte ich** dass meine Transaktionen automatisch kategorisiert werden
**Damit** ich keine Zeit mit manueller Kategorisierung verschwende

**Kontext**:
- Sarah hat 20-30 neue Transaktionen pro Woche
- Aktuell verbringt sie 12 Min./Woche mit Kategorisierung
- Sie checkt die App t√§glich morgens (8-9 Uhr)

**Akzeptanzkriterien**:
- [ ] Neue Transaktionen werden innerhalb 1 Stunde auto-kategorisiert
- [ ] Kategorisierung-Accuracy ‚â• 85% (basierend auf User-Corrections)
- [ ] User sieht Confidence-Level (Hoch/Mittel/Niedrig)
- [ ] User kann Kategorie mit 1 Tap korrigieren
- [ ] Korrektur verbessert zuk√ºnftige Predictions (ML-Feedback-Loop)

**User Flow**:
1. Sarah √∂ffnet App morgens
2. Sieht neue Transaktionen mit Auto-Kategorien
3. Review:
   - ‚úÖ Korrekt ‚Üí Keine Aktion
   - ‚ùå Falsch ‚Üí Tap Kategorie ‚Üí W√§hle neue ‚Üí Best√§tige
4. Dashboard zeigt aktualisierte Zahlen

**Edge Cases**:
- **Neue Merchant**: Wenn Merchant unbekannt ‚Üí "Niedrige Confidence"
- **Zweideutig**: Z.B. "Amazon" (Shopping oder Cloud) ‚Üí Frage User
- **Offline**: Kategorisierung erfolgt wenn online

**Abh√§ngigkeiten**:
- ML-Modell deployed (ML Team, Sprint 0)
- Transaction API updated (Backend, Sprint 1)
- UI Components fertig (Design, Sprint 0)

**Mockups**: [Link to Figma]

**Definition of Done**:
- [ ] Code reviewed & merged
- [ ] Unit Tests (Coverage ‚â• 80%)
- [ ] QA Testing passed
- [ ] Analytics Events implemented
- [ ] Documentation updated
- [ ] Deployed to Production
```

### H√§ufige Fehler

- ‚ùå Personas aus dem Bauch, nicht datenbasiert
- ‚ùå User Stories ohne Akzeptanzkriterien
- ‚ùå Zu technische User Stories ("Als System")
- ‚ùå Fehlender Kontext/Rationale
- ‚ùå Keine Priorisierung

---

## Funktionale Anforderungen

### Zweck

Beschreiben WAS gebaut wird (nicht WIE).

### Struktur nach MoSCoW

- **M**ust-Have: Ohne geht es nicht
- **S**hould-Have: Wichtig, aber nicht kritisch
- **C**ould-Have: Nice-to-Have
- **W**on't-Have: Explizit ausgeschlossen

### Template

```markdown
## Funktionale Anforderungen

### Must-Have (MVP - Ohne geht Launch nicht)

#### FR-1: Automatische Kategorisierung neuer Transaktionen

**Beschreibung**:
System kategorisiert neue Transaktionen automatisch basierend auf
ML-Modell, das auf historischen Daten und User-Korrekturen trainiert wurde.

**Funktionale Details**:
- Kategorisierung erfolgt innerhalb 1 Stunde nach Transaction-Import
- 15 vordefinierte Kategorien (siehe Appendix A)
- Confidence-Level angezeigt: Hoch (>90%), Mittel (70-90%), Niedrig (<70%)
- User kann Kategorie √§ndern mit 1-Tap-Correction
- Korrektur flie√üt in Training-Data (Feedback-Loop)

**Verhalten**:

| Szenario | System-Verhalten |
|----------|------------------|
| Bekannter Merchant | Auto-Kategorisieren mit "Hoch" Confidence |
| √Ñhnlicher Merchant | Auto-Kategorisieren mit "Mittel" Confidence |
| Neuer Merchant | Best-Guess mit "Niedrig" Confidence |
| Ambiguous (z.B. Amazon) | Frage User bei erstem Mal |

**Edge Cases**:

1. **Split-Transactions**: Wenn User Split erstellt, Original-Kategorie
   wird auf Splits √ºbertragen

2. **Bulk-Correction**: User √§ndert Kategorie f√ºr Merchant ‚Üí System
   fragt "Alle fr√ºheren Transaktionen auch √§ndern?"

3. **Offline-Mode**: Transaktionen werden gecached und kategorisiert
   sobald online

**User Interface**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Neue Transaktionen          ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ ‚úì Edeka Supermarkt    üõí   ‚îÇ
‚îÇ   Groceries (Hoch)          ‚îÇ
‚îÇ   -42,50 ‚Ç¨                  ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ ? Amazon.de           üì¶   ‚îÇ
‚îÇ   Shopping (Mittel)         ‚îÇ
‚îÇ   -89,99 ‚Ç¨                  ‚îÇ
‚îÇ   [Kategorie korrekt?]      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Akzeptanzkriterien**:
- [ ] Neue Transaktionen werden auto-kategorisiert
- [ ] Confidence-Level wird angezeigt
- [ ] User kann mit max. 2 Taps korrigieren
- [ ] Korrektur verbessert Model (verified via Testing)
- [ ] Works on iOS & Android
- [ ] Ladezeit < 2 Sekunden

**Non-Goals (f√ºr dieses FR)**:
- ‚ùå Custom User-Kategorien (kommt in Should-Have)
- ‚ùå Sub-Kategorien (Future)
- ‚ùå Regel-basierte Kategorisierung (nur ML)

**Abh√§ngigkeiten**:
- ML-Model deployed & accessible via API
- Transaction-Import funktioniert
- Categories-Master-Data definiert

**Test-Strategie**:
- Unit Tests: Model-Prediction Logic
- Integration: API End-to-End
- E2E: User korrigiert Kategorie ‚Üí N√§chste Transaction same Merchant korrekt
- Performance: 1000 Transactions < 5s

**Mockups**: [Figma Link]

#### FR-2: Kategorie-Korrektur

[...]

### Should-Have (Post-MVP, vor Ende Q2)

#### FR-5: Custom User-Kategorien

**Beschreibung**: User k√∂nnen eigene Kategorien erstellen

**Rationale f√ºr Should-Have**:
- Nicht kritisch f√ºr MVP
- 35% User-Request (nicht Mehrheit)
- Erh√∂ht Complexity (Testing, Migration)
- Kann post-launch hinzugef√ºgt werden

[Details...]

### Could-Have (Backlog, re-evaluate post-launch)

#### FR-8: Kategorie-Regeln

**Beschreibung**: User definiert Regel "Alle Transaktionen von X ‚Üí Kategorie Y"

**Rationale f√ºr Could-Have**:
- Nur 12% User-Request
- Hoher Implementierungsaufwand
- ML-Model should learn from corrections anyway
- Evaluate if needed basierend auf Post-Launch-Feedback

### Won't-Have (Explizit ausgeschlossen)

#### Sub-Kategorien

**Rationale**: Zu komplex f√ºr MVP, User Research zeigt geringen Wert (8% Request)

#### Auto-Tagging

**Rationale**: Separate Feature, eigene PRD in Q3
```

### H√§ufige Fehler

- ‚ùå Zu technisch ("Verwende Redis-Cache")
- ‚ùå Keine Priorit√§ts-Begr√ºndung
- ‚ùå Unklare Akzeptanzkriterien
- ‚ùå Fehlende Edge Cases
- ‚ùå Keine User-Perspektive

---

## Nicht-funktionale Anforderungen (NFRs)

### Zweck

Qualit√§ts-Attribute definieren: Wie gut muss es sein?

### Kategorien

1. **Performance**: Geschwindigkeit, Latenz
2. **Scalability**: Wachstum, Load
3. **Security**: Sicherheit, Privacy
4. **Reliability**: Uptime, Error Rate
5. **Usability**: Benutzerfreundlichkeit
6. **Accessibility**: Barrierefreiheit
7. **Maintainability**: Wartbarkeit (f√ºr Dev-Team)

### Template

```markdown
## Nicht-funktionale Anforderungen

### Performance

**NFR-1: Response Time**
- **Requirement**: 95% der API-Requests < 500ms
- **Rationale**: User erwarten sofortige Antwort, Mobile-First
- **Messung**: APM (Application Performance Monitoring)
- **Testing**: Load Tests mit 1000 concurrent users

**NFR-2: UI Responsiveness**
- **Requirement**: Time-to-Interactive < 2 Sekunden
- **Rationale**: Mobile 3G-Connection Mindest-Standard
- **Messung**: Lighthouse Performance Score > 90
- **Testing**: Real Device Testing, Throttled Network

### Scalability

**NFR-3: User Load**
- **Requirement**: System funktioniert bei 100k DAU
- **Current**: 50k DAU
- **Growth**: +50k expected in Q2
- **Testing**: Load Tests, Stress Tests

**NFR-4: Data Volume**
- **Requirement**: Performant mit 1M+ Transaktionen pro User
- **Rationale**: Power-Users mit mehrj√§hrigen Daten
- **Testing**: Test mit Production-Data-Samples

### Security & Privacy

**NFR-5: Data Encryption**
- **Requirement**: All PII encrypted at rest (AES-256)
- **Compliance**: GDPR, PCI-DSS (if applicable)
- **Audit**: Pentesting vor Launch

**NFR-6: GDPR Compliance**
- **Requirement**: User kann Daten exportieren & l√∂schen
- **Timeline**: Must be ready at Launch (Legal Requirement)
- **Validation**: Legal Review

**NFR-7: ML Model Privacy**
- **Requirement**: Model-Training nur mit anonymisierten Daten
- **Rationale**: Privacy-First, keine User-Identifiables
- **Validation**: Privacy Impact Assessment

### Reliability

**NFR-8: Availability**
- **Requirement**: 99.9% Uptime (< 43 Min. Downtime/Monat)
- **Rationale**: Finance-App, Users checken t√§glich
- **Monitoring**: Uptime Robot, PagerDuty

**NFR-9: Error Rate**
- **Requirement**: < 0.1% Error Rate f√ºr Kategorisierung
- **Rationale**: Trust in Auto-Kategorisierung kritisch
- **Monitoring**: Sentry, Error Tracking

**NFR-10: Data Loss Prevention**
- **Requirement**: Zero data loss
- **Strategy**: Backups, Redundancy
- **RTO/RPO**: Recovery Time < 1h, Recovery Point < 5 Min.

### Usability

**NFR-11: Intuitive UI**
- **Requirement**: 90% der User verstehen Feature ohne Onboarding
- **Validation**: Usability Testing (n ‚â• 10)
- **Metrics**: Task Success Rate, Time-on-Task

**NFR-12: Error Messages**
- **Requirement**: Fehler klar kommuniziert mit Handlungsempfehlung
- **Example**: "Kategorisierung fehlgeschlagen. Bitte pr√ºfe Internetverbindung."
- **Validation**: Review mit UX Writer

### Accessibility

**NFR-13: WCAG 2.1 Compliance**
- **Requirement**: Level AA compliant
- **Rationale**: Inklusives Design, Legal in einigen M√§rkten
- **Testing**:
  - Automated: axe DevTools, Lighthouse
  - Manual: Screen Reader Testing (NVDA, VoiceOver)

**NFR-14: Keyboard Navigation**
- **Requirement**: Alle Funktionen per Keyboard erreichbar
- **Testing**: Manual Keyboard-Only Testing

**NFR-15: Color Contrast**
- **Requirement**: Min. 4.5:1 f√ºr Text, 3:1 f√ºr UI Components
- **Tool**: Color Contrast Analyzer

### Maintainability

**NFR-16: Code Quality**
- **Requirement**: Test Coverage ‚â• 80%
- **Rationale**: Feature wird iteriert, Tests sch√ºtzen
- **Enforcement**: CI/CD Pipeline checks

**NFR-17: Documentation**
- **Requirement**: API documented, Architecture Decision Records
- **Rationale**: Team-Skalierung, Knowledge-Transfer
- **Format**: OpenAPI Spec, ADRs in repo
```

### H√§ufige Fehler

- ‚ùå Vage Anforderungen ("muss schnell sein")
- ‚ùå Keine messbaren Targets
- ‚ùå Unrealistische Anforderungen
- ‚ùå NFRs vergessen (nur auf Funktionalit√§t fokussiert)

---

## Out of Scope / Abgrenzung

### Zweck

Erwartungen managen. Klar kommunizieren was NICHT gebaut wird.

### Template

```markdown
## Abgrenzung (Out of Scope)

### Nicht in diesem Release

| Feature | Rationale | Geplant f√ºr |
|---------|-----------|-------------|
| **Custom User-Kategorien** | Erh√∂ht MVP-Complexity, nur 35% Request | Q2 Post-Launch |
| **Sub-Kategorien** | Geringer User-Wert (8% Request) | Q3 if validated |
| **Bulk-Edit** | Nice-to-Have, nicht kritisch | Backlog |
| **ML-Model Self-Learning** | Technisch komplex, separate Initiative | Q4 Tech Roadmap |

### Explizit ausgeschlossen

- ‚ùå **Automatisches Tagging**: Separate PRD, Q3 Feature
- ‚ùå **Budget-Integration**: Out of Scope f√ºr Kategorisierung
- ‚ùå **Multi-W√§hrung**: Bereits vorhanden, nicht Teil dieser PRD
- ‚ùå **Historical Data Migration**: User-Daten bleiben wie sind

**Rationale**: Focus auf Core-Feature (Auto-Categorization).
MVP-Ansatz f√ºr schnelles Launch & User-Feedback.

### Abgrenzung zu anderen Projekten

- **"Budget-Alerts" PRD**: Nutzt Kategorien, aber separate Initiative
- **"Reports V2" PRD**: Displayed Kategorien, aber nicht Teil dieser PRD

### Future Considerations

Features die VIELLEICHT sp√§ter kommen:

- **AI-Suggested Categories**: ML schl√§gt neue Kategorien vor
- **Kategorie-Marketplace**: User teilen Kategorie-Sets
- **Family-Kategorien**: Gemeinsame Kategorien f√ºr Partner

**Entscheidung**: Re-evaluate nach Launch basierend auf:
- User-Feedback & Feature-Requests
- Usage Analytics
- Business Priorit√§ten
```

### H√§ufige Fehler

- ‚ùå Out-of-Scope nicht dokumentiert
- ‚ùå Scope Creep w√§hrend Development
- ‚ùå Keine Rationale f√ºr Ausschluss
- ‚ùå Unklare Zukunftspl√§ne

---

## Risikobewertung

### Zweck

Proaktiv Probleme identifizieren und Mitigation planen.

### Risiko-Matrix

```
           Impact
         ‚îÇ  Low  ‚îÇ Medium ‚îÇ  High  ‚îÇ
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
High     ‚îÇ   üü®  ‚îÇ   üüß   ‚îÇ   üü•   ‚îÇ Priority
Likelihood‚îÇ       ‚îÇ        ‚îÇ        ‚îÇ
Medium   ‚îÇ   üü©  ‚îÇ   üü®   ‚îÇ   üüß   ‚îÇ
         ‚îÇ       ‚îÇ        ‚îÇ        ‚îÇ
Low      ‚îÇ   üü©  ‚îÇ   üü©   ‚îÇ   üü®   ‚îÇ
```

### Template

```markdown
## Risikobewertung

### Risiko-Matrix

| ID | Risiko | Impact | Likelihood | Priority | Owner |
|----|--------|--------|------------|----------|-------|
| R-1 | ML-Accuracy zu niedrig | üü• High | üü® Medium | üüß | ML Lead |
| R-2 | Performance bei Scale | üüß Medium | üü© Low | üü© | Eng Lead |
| R-3 | Privacy Concerns | üü• High | üü© Low | üü® | PM + Legal |

### Detaillierte Risiko-Analyse

#### R-1: ML-Modell-Accuracy unter Target (üüß High Priority)

**Risiko-Beschreibung**:
ML-Modell erreicht nicht die target 85% Accuracy in Production.
Model wurde mit synthetic data trainiert und k√∂nnte bei echten
User-Daten schlechter performen.

**Impact**: üü• High
- User-Trust in Auto-Kategorisierung sinkt
- Mehr manuelle Korrekturen ‚Üí schlechte UX
- Negative NPS, m√∂glicherweise Churn
- Feature-Adoption < Target

**Likelihood**: üü® Medium
- Model in Testing: 89% Accuracy
- Aber: Test-Data != Production-Data
- Neue Merchants/Edge Cases in Production

**Mitigation-Strategie** (Proaktiv):

1. **Pre-Launch**:
   - [ ] Test mit Production-Data-Sample (anonymisiert)
   - [ ] A/B Test: 10% User Beta (2 Wochen vor Launch)
   - [ ] Confidence-Thresholds kalibrieren

2. **Launch**:
   - [ ] Phased Rollout: 1% ‚Üí 10% ‚Üí 50% ‚Üí 100%
   - [ ] Real-time Model Monitoring (Accuracy, Confidence-Distribution)
   - [ ] Weekly Model-Retraining mit User-Corrections

3. **Post-Launch**:
   - [ ] User-Feedback-Loop: "War diese Kategorisierung hilfreich?"
   - [ ] Manual Review von Low-Confidence Predictions
   - [ ] Continuous Model Improvement

**Contingency Plan** (Falls es eintritt):

- **Trigger**: Accuracy < 80% f√ºr 3 Tage consecutive
- **Action**:
  1. Feature-Flag OFF f√ºr neue Users (bestehende k√∂nnen weiter nutzen)
  2. Emergency Model-Retraining mit production data
  3. Bring in ML-Expert f√ºr Deep Dive
  4. Communication: Transparent mit Users ("Wir verbessern noch")
- **Timeline**: 5 Tage f√ºr Fix
- **Rollback**: Falls nicht fixable in 1 Woche ‚Üí Rollback, re-plan

**Owner**: ML Lead (Primary), PM (Secondary)

**Status**: Mitigation in Progress (Pre-Launch Testing l√§uft)

---

#### R-2: Performance-Degradation bei High Scale (üü© Low Priority)

**Risiko**: System langsam bei 100k+ concurrent categorization requests

**Impact**: üüß Medium
- User-Experience leidet (langsame Kategorisierung)
- Potentiell Timeouts
- Negative Impact auf NFR-1 (Response Time)

**Likelihood**: üü© Low
- Load-Tests zeigen Performance OK bis 150k users
- Current: 50k DAU, Growth zu 100k dauert 6+ Monate
- Zeit f√ºr Skalierung falls n√∂tig

**Mitigation**:
- Load Tests im CI/CD
- Auto-Scaling konfiguriert
- Performance-Monitoring (APM)
- Fallback: Async Kategorisierung falls Load hoch

**Owner**: Engineering Lead

---

#### R-3: Privacy/GDPR Concerns (üü® Medium Priority)

**Risiko**: User besorgt √ºber Datennutzung f√ºr ML-Training

**Impact**: üü• High (wenn eintritt)
- PR-Problem
- Trust-Loss
- Potentiell Legal Issues

**Likelihood**: üü© Low
- Privacy Impact Assessment durchgef√ºhrt
- Legal Sign-off erhalten
- Transparent kommuniziert in Privacy Policy

**Mitigation**:
- Privacy-Notice vor Feature-Activation
- Opt-Out-Option verf√ºgbar
- Model-Training nur mit anonymisierten Daten
- Clear Communication im UI

**Owner**: PM + Legal Lead

### Risk Review Schedule

- **Pre-Launch**: Weekly Risk Review (PM, Eng Lead, ML Lead)
- **Launch Week**: Daily Monitoring
- **Post-Launch**: Bi-Weekly Review bis Metrics stable
```

### H√§ufige Fehler

- ‚ùå Risks nicht dokumentiert
- ‚ùå Keine Mitigation-Pl√§ne
- ‚ùå Unrealistische Risk-Assessment
- ‚ùå Kein Owner assigned
- ‚ùå Keine Contingency-Pl√§ne

---

## Weitere wichtige Abschnitte

### Timeline & Meilensteine

- Realistische Zeitsch√§tzungen
- Puffer einplanen (15-20%)
- Dependencies ber√ºcksichtigen
- Milestones klar definiert

### Anhang

- Mockups/Wireframes
- Technical Specs (Links)
- Research Reports (Links)
- Competitive Analysis
- Glossar f√ºr Fach-Begriffe

### Approval & Sign-off

- Alle Stakeholder gelistet
- Clear Approval-Process
- Timeline f√ºr Reviews
- Dokumentierte Approvals
